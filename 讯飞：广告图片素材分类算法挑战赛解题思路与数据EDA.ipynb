{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 讯飞：广告图片素材分类算法挑战赛解题思路与数据EDA\n",
    "\n",
    "\n",
    "\n",
    "### 一、赛题背景\n",
    "\n",
    "\n",
    "\n",
    "讯飞AI营销是科大讯飞集团在数字广告领域发展的重要业务，基于深耕多年的人工智能技术和大数据积累，赋予营销智慧创新的大脑，以健全的产品矩阵和全方位的服务，帮助广告主用AI技术实现营销效能的全面提升，打造数字营销新生态。\n",
    "目前，AI营销平台除自有媒体外，已在功能社交、休闲娱乐、专业进阶、衣食住行等类型的TOP媒体实现规模化覆盖，并且覆盖媒体数量仍在快速增长。如何确保在成百上千的媒体上投放符合要求的广告素材（例如，教育类APP需要投放适合青少年的广告内容），是素材审核迫切需要解决的问题。而作为最常见的广告素材类型之一，图片的自动分类将会极大提高审核效率。\n",
    "\n",
    "### 二、赛事任务\n",
    "本次分类算法任务中，讯飞AI营销将提供海量现网流量中的广告图片素材作为训练数据，参赛选手基于提供的训练数据构建模型，实现自动化广告图片素材分类。\n",
    "\n",
    "\n",
    "### 三、数据说明\n",
    "本次比赛为参赛选手提供的数据分为训练集、测试集、提交样例三类文件：\n",
    "\n",
    "训练集：包含10万+广告素材图片，100多个类别，几十种图片尺寸；且图片已按类别放在不同文件夹内，文件夹名称即为素材图片的category_id。\n",
    "\n",
    "测试集：包含1万张广告素材图片，放在同一个文件夹内，图片文件的名称即为image_id。\n",
    "\n",
    "提交样例：表头为image_id和category_id的CSV文件，选手提交数据时需要将测试集的图片id与模型预测的类别id按样例格式填入CSV中，进行提交。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据EDA\n",
    "\n",
    "\n",
    "探索性数据分析（Exploratory Data Analysis，简称EDA），是指对已有的数据（原始数据）进行分析探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方法。一般来说，我们最初接触到数据的时候往往是毫无头绪的，不知道如何下手，这时候探索性数据分析就非常有效。\n",
    "\n",
    "\n",
    "当我们拿到数据的时候，首先应该观察数据的分布情况，一般来说，图像分类任务的赛题通常是在类别不平衡上做文章。那么我们来看看这个数据集的数据分布情况。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据集解压\r\n",
    "!cd 'data/data98270' && unzip -q train.zip\r\n",
    "\r\n",
    "!cd 'data/data98441' && unzip -q test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 数据EDA\r\n",
    "\r\n",
    "import os\r\n",
    "path = 'data/data98270/train'  #父文件夹路径\r\n",
    "all_folds = os.listdir(path)   #解析出父文件夹中所有的文件名称，并以列表的格式输出，\r\n",
    "#例如['add','common-mobile-web-app-icons.zip', '新建 Microsoft Word 文档.docx', 'arrow_down']\r\n",
    "l = []\r\n",
    "for i in range(len(all_folds)):\r\n",
    "    fold_path = os.path.join(path,all_folds[i])  #将父文件夹路径加上子文件的名称，例如：'D:/testin/common-mobile-web-app-icons/add'\r\n",
    "    if os.path.isdir(fold_path):   #判断该文件是否为文件夹\r\n",
    "        count_fold = len(os.listdir(fold_path))\r\n",
    "        #print(all_folds[i],count_fold)\r\n",
    "        l.append((all_folds[i],count_fold))  #得到列表，列表里面是数组，数组里面是文件名称和该文件名称文件夹中图片个数\r\n",
    "#print(l)\r\n",
    "dic_file = dict(l)  #数组转成字典\r\n",
    "#dic_file\r\n",
    "txt_file = os.getcwd()+'\\count.txt'  #os.getcwd()得到当前路径，并在当前路径下建一个txt文本文件\r\n",
    "out = open(txt_file,'w')  #打开文本文件\r\n",
    "for i in  dic_file:  #循环字典的键\r\n",
    "    # out.write(i)  #写入键，既文件夹名称\r\n",
    "    # out.write(': ')\r\n",
    "    out.write(str(dic_file[i]))  #写入值，既文件夹名称下的图片个数\r\n",
    "    out.write('\\n')  #换行\r\n",
    "out.close()  #关闭txt文本文件\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "\r\n",
    "df = pd.read_table('count.txt',header=None,index_col=False)\r\n",
    "\r\n",
    "# print(df[0])\r\n",
    "data = df[0].sort_values()\r\n",
    "\r\n",
    "plt.bar(range(len(data)), data)\r\n",
    "\r\n",
    "plt.savefig(\"eda.jpg\")\r\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/e0c07a51194642bc89eeedc35de821af759433122e4b444685a6b70f5cd92a6b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* **解题思路**\n",
    "\n",
    "\n",
    "**label shuffling**\n",
    "\n",
    "&emsp;&emsp;首先对原始的图像列表，按照标签顺序进行排序；\n",
    "然后计算每个类别的样本数量，并得到样本最多的那个类别的样本数。\n",
    "根据这个最多的样本数，对每类都产生一个随机排列的列表；\n",
    "然后用每个类别的列表中的数对各自类别的样本数求余，得到一个索引值，从该类的图像中提取图像，生成该类的图像随机列表；\n",
    "然后把所有类别的随机列表连在一起，做个Random Shuffling，得到最后的图像列表，用这个列表进行训练。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/36dc85bab2c84602a04dec6fe7db3914a96c66546a7b4bb68f61b8eb77387c35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**数据预处理**\n",
    "\n",
    "\n",
    "此部分对数据进行预处理，生成我们需要的CSV文件。另外实现label shuffling策略，此处我们选择ResNet_vd版本，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "import json\r\n",
    "import cv2\r\n",
    "\r\n",
    "rootDir = 'data/data98270/train'\r\n",
    "\r\n",
    "list = []\r\n",
    "\r\n",
    "image_id = []\r\n",
    "category_id = []\r\n",
    "\r\n",
    "def Test1(rootDir):\r\n",
    "    list_dirs = os.walk(rootDir)\r\n",
    "   \r\n",
    "\r\n",
    "    img_id = 0\r\n",
    "    for root, dirs, files in list_dirs:\r\n",
    "        for d in dirs:\r\n",
    "            # print(os.path.join(root, d))\r\n",
    "            path = os.path.join(root, d)\r\n",
    "            cat_id = int(d)\r\n",
    "            for im in os.listdir(path):\r\n",
    "                dict = {}\r\n",
    "                img = cv2.imread(os.path.join(path, im))\r\n",
    "                # print(os.path.join(path, im))\r\n",
    "                img_h = img.shape[0]\r\n",
    "                img_w = img.shape[1]\r\n",
    "                dict['image_id'] = img_id\r\n",
    "                img_id += 1\r\n",
    "                dict['fpath'] = os.path.join(path, im)\r\n",
    "                dict['im_height'] = img_h\r\n",
    "                dict['im_width'] = img_w\r\n",
    "                dict['category_id'] = cat_id\r\n",
    "                list.append(dict)\r\n",
    "                image_id.append(d+'/'+im)\r\n",
    "                category_id.append(cat_id)\r\n",
    "\r\n",
    "Test1(rootDir)\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "img = pd.DataFrame(image_id)\r\n",
    "img = img.rename(columns = {0:\"image_id\"})\r\n",
    "img['category_id'] = category_id\r\n",
    "\r\n",
    "img.to_csv('train.csv', index=False)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入所需要的库\r\n",
    "from sklearn.utils import shuffle\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from paddle.io import Dataset\r\n",
    "import paddle.vision.transforms as T\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.metric import Accuracy\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "# 读取数据\r\n",
    "train_images = pd.read_csv('train.csv')\r\n",
    "\r\n",
    "# labelshuffling\r\n",
    "\r\n",
    "def labelShuffling(dataFrame, groupByName = 'category_id'):\r\n",
    "\r\n",
    "    groupDataFrame = dataFrame.groupby(by=[groupByName])\r\n",
    "    labels = groupDataFrame.size()\r\n",
    "    print(\"length of label is \", len(labels))\r\n",
    "    maxNum = max(labels)\r\n",
    "    lst = pd.DataFrame()\r\n",
    "    for i in range(len(labels)):\r\n",
    "        print(\"Processing label  :\", i)\r\n",
    "        tmpGroupBy = groupDataFrame.get_group(i)\r\n",
    "        print(labels[2])\r\n",
    "\r\n",
    "        createdShuffleLabels = np.random.permutation(np.array(range(maxNum))) % labels[i]\r\n",
    "        print(\"Num of the label is : \", labels[i])\r\n",
    "        lst=lst.append(tmpGroupBy.iloc[createdShuffleLabels], ignore_index=True)\r\n",
    "        print(\"Done\")\r\n",
    "    # lst.to_csv('test1.csv', index=False)\r\n",
    "    return lst\r\n",
    "\r\n",
    "# 划分训练集和校验集\r\n",
    "all_size = len(train_images)\r\n",
    "# print(all_size)\r\n",
    "train_size = int(all_size * 0.9)\r\n",
    "\r\n",
    "train_images = shuffle(train_images)\r\n",
    "\r\n",
    "train_image_list = train_images[:train_size]\r\n",
    "val_image_list = train_images[train_size:]\r\n",
    "\r\n",
    "# df = labelShuffling(train_image_list)\r\n",
    "\r\n",
    "# df = shuffle(df)\r\n",
    "df = train_image_list\r\n",
    "train_image_path_list = df['image_id'].values\r\n",
    "label_list = df['category_id'].values\r\n",
    "label_list = paddle.to_tensor(label_list, dtype='int64')\r\n",
    "train_label_list = paddle.nn.functional.one_hot(label_list, num_classes=137)\r\n",
    "\r\n",
    "val_image_path_list = val_image_list['image_id'].values\r\n",
    "val_label_list = val_image_list['category_id'].values\r\n",
    "val_label_list = paddle.to_tensor(val_label_list, dtype='int64')\r\n",
    "val_label_list = paddle.nn.functional.one_hot(val_label_list, num_classes=137)\r\n",
    "\r\n",
    "# 定义数据预处理\r\n",
    "data_transforms = T.Compose([\r\n",
    "    T.Resize(size=(224, 224)),\r\n",
    "    T.RandomHorizontalFlip(224),\r\n",
    "    T.RandomVerticalFlip(224),\r\n",
    "    T.Transpose(),    # HWC -> CHW\r\n",
    "    T.Normalize(\r\n",
    "        mean=[0, 0, 0],        # 归一化\r\n",
    "        std=[255, 255, 255],\r\n",
    "        to_rgb=True)    \r\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建Dataset\r\n",
    "class MyDataset(paddle.io.Dataset):\r\n",
    "    \"\"\"\r\n",
    "    步骤一：继承paddle.io.Dataset类\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, train_img_list, val_img_list,train_label_list,val_label_list, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集\r\n",
    "        \"\"\"\r\n",
    "        super(MyDataset, self).__init__()\r\n",
    "        self.img = []\r\n",
    "        self.label = []\r\n",
    "        # 借助pandas读csv的库\r\n",
    "        self.train_images = train_img_list\r\n",
    "        self.test_images = val_img_list\r\n",
    "        self.train_label = train_label_list\r\n",
    "        self.test_label = val_label_list\r\n",
    "        if mode == 'train':\r\n",
    "            # 读train_images的数据\r\n",
    "            for img,la in zip(self.train_images, self.train_label):\r\n",
    "                self.img.append('data/data98270/train/'+img)\r\n",
    "                self.label.append(la)\r\n",
    "        else:\r\n",
    "            # 读test_images的数据\r\n",
    "            for img,la in zip(self.train_images, self.train_label):\r\n",
    "                self.img.append('data/data98270/train/'+img)\r\n",
    "                self.label.append(la)\r\n",
    "\r\n",
    "    def load_img(self, image_path):\r\n",
    "        # 实际使用时使用Pillow相关库进行图片读取即可，这里我们对数据先做个模拟\r\n",
    "        image = Image.open(image_path).convert('RGB')\r\n",
    "        return image\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\r\n",
    "        \"\"\"\r\n",
    "        image = self.load_img(self.img[index])\r\n",
    "        label = self.label[index]\r\n",
    "        # label = paddle.to_tensor(label)\r\n",
    "        \r\n",
    "        return data_transforms(image), paddle.nn.functional.label_smooth(label)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \"\"\"\r\n",
    "        步骤四：实现__len__方法，返回数据集总数目\r\n",
    "        \"\"\"\r\n",
    "        return len(self.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_loader\r\n",
    "train_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='train')\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), batch_size=128, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "#val_loader\r\n",
    "\r\n",
    "val_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='test')\r\n",
    "val_loader = paddle.io.DataLoader(val_dataset, places=paddle.CPUPlace(), batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 调用飞桨框架的VisualDL模块，保存信息到目录中。\r\n",
    "callback = paddle.callbacks.VisualDL(log_dir='visualdl_log_dir')\r\n",
    "\r\n",
    "from visualdl import LogReader, LogWriter\r\n",
    "\r\n",
    "args={\r\n",
    "    'logdir':'./vdl',\r\n",
    "    'file_name':'vdlrecords.model.log',\r\n",
    "    'iters':0,\r\n",
    "}\r\n",
    "\r\n",
    "# 配置visualdl\r\n",
    "write = LogWriter(logdir=args['logdir'], file_name=args['file_name'])\r\n",
    "#iters 初始化为0\r\n",
    "iters = args['iters'] \r\n",
    "\r\n",
    "#自定义Callback\r\n",
    "class Callbk(paddle.callbacks.Callback):\r\n",
    "    def __init__(self, write, iters=0):\r\n",
    "        self.write = write\r\n",
    "        self.iters = iters\r\n",
    "\r\n",
    "    def on_train_batch_end(self, step, logs):\r\n",
    "\r\n",
    "        self.iters += 1\r\n",
    "\r\n",
    "        #记录loss\r\n",
    "        self.write.add_scalar(tag=\"loss\",step=self.iters,value=logs['loss'][0])\r\n",
    "        #记录 accuracy\r\n",
    "        self.write.add_scalar(tag=\"acc\",step=self.iters,value=logs['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from work.res2net import Res2Net50_vd_26w_4s\r\n",
    "\r\n",
    "# 模型封装\r\n",
    "model_res = Res2Net50_vd_26w_4s(class_dim=137)\r\n",
    "\r\n",
    "model = paddle.Model(model_res)\r\n",
    "\r\n",
    "paddle.summary(mnist, (64, 3, 224, 224))\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=3e-4, parameters=model.parameters())\r\n",
    "\r\n",
    "# 配置模型\r\n",
    "model.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(soft_label=True),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "\r\n",
    "model.load('work/Res2Net50_vd_26w_4s_pretrained.pdparams',skip_mismatch=True)\r\n",
    "\r\n",
    "# 模型训练与评估\r\n",
    "model.fit(train_loader,\r\n",
    "        val_loader,\r\n",
    "        log_freq=1,\r\n",
    "        epochs=10,\r\n",
    "        callbacks=Callbk(write=write, iters=iters),\r\n",
    "        verbose=1,\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存模型参数\r\n",
    "# model.save('Hapi_MyCNN_resume')  # save for training\r\n",
    "model.save('Hapi_MyCNN1', False)  # save for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "from paddle.vision import transforms\r\n",
    "\r\n",
    "def load_image(img_path):\r\n",
    "    '''\r\n",
    "    预测图片预处理\r\n",
    "    '''\r\n",
    "    img = Image.open(img_path).convert('RGB')\r\n",
    "    \r\n",
    "    #resize\r\n",
    "    img = img.resize((224, 224), Image.BILINEAR) #Image.BILINEAR双线性插值\r\n",
    "    img = np.array(img).astype('float32')\r\n",
    "\r\n",
    "    # HWC to CHW \r\n",
    "    img = img.transpose((2, 0, 1))\r\n",
    "    \r\n",
    "    #Normalize\r\n",
    "    img = img / 255         #像素值归一化\r\n",
    "    # print(img)\r\n",
    "    # mean = [0.31169346, 0.25506335, 0.12432463]   \r\n",
    "    # std = [0.34042713, 0.29819837, 0.1375536]\r\n",
    "    # img[0] = (img[0] - mean[0]) / std[0]\r\n",
    "    # img[1] = (img[1] - mean[1]) / std[1]\r\n",
    "    # img[2] = (img[2] - mean[2]) / std[2]\r\n",
    "    \r\n",
    "    return img\r\n",
    "use_gpu = False\r\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\r\n",
    "model = paddle.jit.load(\"Hapi_MyCNN1\")\r\n",
    "def read_img(path):\r\n",
    "\r\n",
    "    img = Image.open(path)\r\n",
    "    # img = img.convert(\"RGB\")\r\n",
    "    transform_valid = transforms.Compose([\r\n",
    "        transforms.Resize((224, 224)),\r\n",
    "        transforms.ToTensor(),\r\n",
    "        transforms.Normalize(\r\n",
    "            mean=[0, 0, 0],  # 归一化\r\n",
    "            std=[255, 255, 255],\r\n",
    "            to_rgb=True)\r\n",
    "    ])\r\n",
    "    img_tensor = transform_valid(img).unsqueeze(0)\r\n",
    "\r\n",
    "    return img_tensor\r\n",
    "\r\n",
    "def infer_img(path, model):\r\n",
    "    '''\r\n",
    "    模型预测\r\n",
    "    '''\r\n",
    "\r\n",
    "    model.eval() \r\n",
    "\r\n",
    "  \r\n",
    "    data = read_img(path)\r\n",
    "    \r\n",
    "    out = model(data)\r\n",
    "    # print(out[0])\r\n",
    "    # print(paddle.nn.functional.softmax(out)[0]) # 若模型中已经包含softmax则不用此行代码。\r\n",
    "\r\n",
    "    lab = np.argmax(out.numpy())  #argmax():返回最大数的索引\r\n",
    "    # label_pre.append(lab)\r\n",
    "       \r\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_list = os.listdir('data/data98441/test/')\r\n",
    "img_list.sort(key=lambda x: int(x[1:-4]))\r\n",
    "# img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_list = []\r\n",
    "\r\n",
    "for i in range(len(img_list)):\r\n",
    "    pre_list.append(infer_img(path='data/data98441/test/' + img_list[i], model=model))\r\n",
    "    # print(i)\r\n",
    "    # time.sleep(0.5) #防止输出错乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "img = pd.DataFrame(img_list)\r\n",
    "img = img.rename(columns = {0:\"image_id\"})\r\n",
    "img['category_id'] = pre_list\r\n",
    "\r\n",
    "img.to_csv('submit123.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 另一个解题思路\n",
    "\n",
    "\n",
    "通过上面的柱状图，我们可以发现，这个数据集就是长尾分布。我们非常明显的知道，使用常规的图像分类网络应该是不可以的，所以我开了一个脑洞，能不能用两个网络，其中一个负责数量较多的那几个类别，另一个负责数量较少的那几个类别，最后再用一个全连接层把两个合在一起，这不就行了嘛！！！！于是我翻出了我去年的项目，[基于Densenet&Xception融合的102种鲜花识别](https://aistudio.baidu.com/aistudio/projectdetail/409180)，正当我沾沾自喜的时候，我发现有人已经研究了这个问题，并且还发了论文。。。所以我的顶会梦就这样碎了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/1cb02138142847efabfc0be53b49355f670199ed29914b2dac2c33909da81854)\n",
    "\n",
    "\n",
    "\n",
    "&emsp;&emsp;针对长尾分布中的不平衡分类问题，本文首次发现这些重新平衡方法能够实现令人满意的识别精度，这是因为它们可以显着地促进深度网络的分类学习。但是同时它们也在一定程度上破坏了学习到的深度特征的代表能力。 因此，本文提出了一个统一的双边分支网络(BBN)来同时处理表示学习和分类器学习，其中每个分支分别各自执行自己的职责 特别是，我们的BBN模型进一步配备了一种新的累积学习策略，其目的是首先学习通用模式，然后逐渐关注尾部数据。\n",
    "       \n",
    "       \n",
    "       \n",
    "&emsp;&emsp;工作初步首先做了验证实验，将深度网络的训练过程分为两步，分别执行特征学习和分类器学习。在第一个特征学习阶段，分别采用传统交叉熵训练方法，重加权和重采样去获得对应的特征表示。然后再第二个分类阶段，我们首先固定特征学习阶段的参数，然后训练这些网络的分类器（也就是全连接层），也是用第一阶段的三种方法。结果显示，当固定特征表示方式时，重平衡方法达到较低的错误率，表明他们能提升分类器学习。另一方面，当固定分类器学习方式， 对原始不平衡数据进行传统方法训练，可以根据其更好的特征带来更好的效果。 此外，重新平衡方法的更糟糕的结果证明了它们将损害特征学习。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 总结\n",
    "\n",
    "\n",
    "针对该类型比赛,大家在选定了一个baseline之后可以尝试各种技巧，包括学习率调整策略，模型调参等等。关于图像分类竞赛的一些技巧，大家可以查看我之前另一个项目，[图像分类竞赛技巧实战]。(https://aistudio.baidu.com/aistudio/projectdetail/1551646)  关于BBN这个网络，目前还没有基于飞桨实现的版本，反正我是没有发现。。。大家可以去github上拉取一下官方代码\n",
    "\n",
    "\n",
    "**建议**\n",
    "\n",
    "* 小白入坑，可独立完成一个比赛，不追求名次，但要渴望追求学习新知识。比赛开始优先使用最简单的模型（如ResNet18），快速跑完整个训练和预测流程。\n",
    "* 要有一定毅力，不怕失败，比赛过程往往会踩到不少坑。数据扩增方法一定要反复尝试，会很大程度上影响模型精度。\n",
    "* 有充足的时间，看相关论文，找灵感，有些domain的知识是必须有个基本概念认识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
