{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 讯飞农作物生长情况识别挑战赛baseline(非官方)\n",
    "\n",
    "\n",
    "\n",
    "### 一、赛事背景\n",
    "随着物联网和大数据技术的飞速发展，智慧农业已经是智慧经济的重要组成部分。我国是农业大国，而非农业强国，因此要实现高水平的设施农业生产和优化设施生物环境控制，农业相关信息获取、分析技术是农业生产中最关键的技术之一。\n",
    "\n",
    "农作物的生长态势，事关农事生产的整个过程，因此通过农作物不同时期图片，对农作物进行合理的农作物生产态势检测，对于农业生产是十分有必要的。通过分析农作物生长情况，最大程度地判断农作物生长态势，合理调配生产资源，为农作物生产管理人员或管理决策者提供及时可靠的长势信息，便于及时采集有效的田间管理措施，对农作物产量进行准确预估，为我国人民的生存条件和粮食安全提供保障。\n",
    "\n",
    "### 二、赛事任务\n",
    "通过作物不同生长时期的特点可以对作物的生长情况进行识别，给出合理的作物生长阶段。本次大赛提供了大量草莓植株在营养生长阶段的生长情况图片作为样本，参赛选手需基于提供的样本构建模型，对草莓样本生长态势进行检测，判断其生长情况，并将生长情况在csv文件中对应标定出来，给出草莓图像对应的生长阶段。\n",
    "\n",
    "初赛一阶段：6月21日到8月18日。\n",
    "\n",
    "初赛二阶段：8月19日到8月20日。系统将在8月19日00：00更换测试数据，参赛队伍需要再次下载数据。\n",
    "\n",
    "### 三、评审规则\n",
    "1.数据说明\n",
    "本次比赛将会为选手提供草莓处于营养生长阶段的图片作为数据集，其中包含有作物图片及生长情况标签。选手根据训练集进行训练，对测试集数据进行标定，判断所标定的作物处于何种生长情况之下。草莓生长阶段大致可以分为以下几个阶段：生长期、开花期、结果期、成熟期。\n",
    "\n",
    "0，草莓生长期\n",
    "\n",
    "1，草莓开花期\n",
    "\n",
    "2，草莓结果期\n",
    "\n",
    "3，草莓成熟期\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  题目特点及常用方法\n",
    "\n",
    "分析赛题，细粒度图像分类 (Fine-grained Image Categorization), 又被称作子类别图像分类 (Sub-category Recognition)。 其目的是对属于同一基础类别的图像进行更加细致的子类划分, 但由于子类别间细微的类间差异以及较大的类内差异, 与传统的图像分类任务相比, 细粒度图像分类难度明显要大很多。从下图中的草莓图像可以看出，不同病变情况的叶子长相非常相似，此外同一类别由于姿态、背景以及拍摄角度的不同，也存在较大的类内差异。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据集解压\r\n",
    "\r\n",
    "!cd 'data/data97595' && unzip -q nongzuowu.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据EDA\n",
    "\n",
    "\n",
    "&emsp;&emsp;探索性数据分析（Exploratory Data Analysis，简称EDA），是指对已有的数据（原始数据）进行分析探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方法。一般来说，我们最初接触到数据的时候往往是毫无头绪的，不知道如何下手，这时候探索性数据分析就非常有效。\n",
    "\n",
    "  对于图像分类任务，我们通常首先应该统计出每个类别的数量，查看训练集的数据分布情况。通过数据分布情况分析赛题，形成解题思路。（洞察数据的本质很重要。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "df = pd.read_csv('data/data97595/nongzuowu/train.csv')\r\n",
    "d=df['label'].hist().get_figure()\r\n",
    "d.savefig('2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f5986310311f4ff8811e5ef8a21e42bb5d42fac122e84169950d0e38d2ac9af4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## baseline搭建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normMean = [0.4494708, 0.45852903, 0.29217097]\n",
      "normStd = [0.2794664, 0.2744329, 0.2591805]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\r\n",
    "import cv2\r\n",
    "import os\r\n",
    " \r\n",
    "img_h, img_w = 224, 224   #根据自己数据集适当调整，影响不大\r\n",
    "means, stdevs = [], []\r\n",
    "img_list = []\r\n",
    " \r\n",
    "imgs_path = 'data/data97595/nongzuowu/test/testA'\r\n",
    "imgs_path_list = os.listdir(imgs_path)\r\n",
    " \r\n",
    "len_ = len(imgs_path_list)\r\n",
    "i = 0\r\n",
    "for item in imgs_path_list:\r\n",
    "    img = cv2.imread(os.path.join(imgs_path,item))\r\n",
    "    img = cv2.resize(img,(img_w,img_h))\r\n",
    "    img = img[:, :, :, np.newaxis]\r\n",
    "    img_list.append(img)\r\n",
    "    i += 1\r\n",
    "    # print(i,'/',len_)\r\n",
    "\r\n",
    "imgs_path = 'data/data97595/nongzuowu/test/testA'\r\n",
    "imgs_path_list = os.listdir(imgs_path)\r\n",
    " \r\n",
    "len_ = len(imgs_path_list)\r\n",
    "i = 0\r\n",
    "for item in imgs_path_list:\r\n",
    "    img = cv2.imread(os.path.join(imgs_path,item))\r\n",
    "    img = cv2.resize(img,(img_w,img_h))\r\n",
    "    img = img[:, :, :, np.newaxis]\r\n",
    "    img_list.append(img)\r\n",
    "    i += 1\r\n",
    "    # print(i,'/',len_)\r\n",
    "\r\n",
    "# print(len(img_list))\r\n",
    "\r\n",
    "imgs = np.concatenate(img_list, axis=3)\r\n",
    "imgs = imgs.astype(np.float32) / 255.\r\n",
    " \r\n",
    "for i in range(3):\r\n",
    "    pixels = imgs[:, :, i, :].ravel()  # 拉成一行\r\n",
    "    means.append(np.mean(pixels))\r\n",
    "    stdevs.append(np.std(pixels))\r\n",
    " \r\n",
    "# BGR --> RGB ， CV读取的需要转换，PIL读取的不用转换\r\n",
    "means.reverse()\r\n",
    "stdevs.reverse()\r\n",
    " \r\n",
    "print(\"normMean = {}\".format(means))\r\n",
    "print(\"normStd = {}\".format(stdevs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of label is  4\n",
      "Processing label  : 0\n",
      "Num of the label is :  132\n",
      "Done\n",
      "Processing label  : 1\n",
      "Num of the label is :  119\n",
      "Done\n",
      "Processing label  : 2\n",
      "Num of the label is :  119\n",
      "Done\n",
      "Processing label  : 3\n",
      "Num of the label is :  103\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 导入所需要的库\r\n",
    "from sklearn.utils import shuffle\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "from paddle.io import Dataset\r\n",
    "import paddle.vision.transforms as T\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.metric import Accuracy\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "\r\n",
    "# 读取数据\r\n",
    "train_images = pd.read_csv('data/data97595/nongzuowu/train.csv')\r\n",
    "\r\n",
    "# labelshuffling\r\n",
    "\r\n",
    "def labelShuffling(dataFrame, groupByName = 'label'):\r\n",
    "\r\n",
    "    groupDataFrame = dataFrame.groupby(by=[groupByName])\r\n",
    "    labels = groupDataFrame.size()\r\n",
    "    print(\"length of label is \", len(labels))\r\n",
    "    maxNum = max(labels)\r\n",
    "    lst = pd.DataFrame()\r\n",
    "    for i in range(len(labels)):\r\n",
    "        print(\"Processing label  :\", i)\r\n",
    "        tmpGroupBy = groupDataFrame.get_group(i)\r\n",
    "        createdShuffleLabels = np.random.permutation(np.array(range(maxNum))) % labels[i]\r\n",
    "        print(\"Num of the label is : \", labels[i])\r\n",
    "        lst=lst.append(tmpGroupBy.iloc[createdShuffleLabels], ignore_index=True)\r\n",
    "        print(\"Done\")\r\n",
    "    # lst.to_csv('test1.csv', index=False)\r\n",
    "    return lst\r\n",
    "\r\n",
    "# 划分训练集和校验集\r\n",
    "all_size = len(train_images)\r\n",
    "# print(all_size)\r\n",
    "train_size = int(all_size * 0.85)\r\n",
    "train_image_list = train_images[:train_size]\r\n",
    "val_image_list = train_images[train_size:]\r\n",
    "\r\n",
    "df = labelShuffling(train_image_list)\r\n",
    "df = shuffle(df)\r\n",
    "\r\n",
    "train_image_path_list = df['image'].values\r\n",
    "label_list = df['label'].values\r\n",
    "label_list = paddle.to_tensor(label_list, dtype='int64')\r\n",
    "train_label_list = paddle.nn.functional.one_hot(label_list, num_classes=4)\r\n",
    "\r\n",
    "val_image_path_list = val_image_list['image'].values\r\n",
    "val_label_list = val_image_list['label'].values\r\n",
    "val_label_list = paddle.to_tensor(val_label_list, dtype='int64')\r\n",
    "val_label_list = paddle.nn.functional.one_hot(val_label_list, num_classes=4)\r\n",
    "\r\n",
    "# 定义数据预处理\r\n",
    "data_transforms = T.Compose([\r\n",
    "    T.Resize(size=(224, 224)),\r\n",
    "    T.RandomHorizontalFlip(0.5),\r\n",
    "    T.RandomVerticalFlip(0.5),\r\n",
    "    T.RandomRotation(30),\r\n",
    "    T.Transpose(),    # HWC -> CHW\r\n",
    "    T.Normalize(\r\n",
    "        mean=[0.44381204, 0.46818522, 0.30317423],        # 归一化\r\n",
    "        std=[0.27267984, 0.26876768, 0.25462458],\r\n",
    "        to_rgb=True)    \r\n",
    "])\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建Dataset\r\n",
    "class MyDataset(paddle.io.Dataset):\r\n",
    "    \"\"\"\r\n",
    "    步骤一：继承paddle.io.Dataset类\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, train_img_list, val_img_list,train_label_list,val_label_list, mode='train'):\r\n",
    "        \"\"\"\r\n",
    "        步骤二：实现构造函数，定义数据读取方式，划分训练和测试数据集\r\n",
    "        \"\"\"\r\n",
    "        super(MyDataset, self).__init__()\r\n",
    "        self.img = []\r\n",
    "        self.label = []\r\n",
    "        # 借助pandas读csv的库\r\n",
    "        self.train_images = train_img_list\r\n",
    "        self.test_images = val_img_list\r\n",
    "        self.train_label = train_label_list\r\n",
    "        self.test_label = val_label_list\r\n",
    "        if mode == 'train':\r\n",
    "            # 读train_images的数据\r\n",
    "            for img,la in zip(self.train_images, self.train_label):\r\n",
    "                self.img.append('data/data97595/nongzuowu/train/'+img)\r\n",
    "                self.label.append(la)\r\n",
    "        else:\r\n",
    "            # 读test_images的数据\r\n",
    "            for img,la in zip(self.test_images, self.test_label):\r\n",
    "                self.img.append('data/data97595/nongzuowu/train/'+img)\r\n",
    "                self.label.append(la)\r\n",
    "\r\n",
    "    def load_img(self, image_path):\r\n",
    "        # 实际使用时使用Pillow相关库进行图片读取即可，这里我们对数据先做个模拟\r\n",
    "        image = Image.open(image_path).convert('RGB')\r\n",
    "        image = np.asarray(image)\r\n",
    "        image = image.astype(np.float32) / 255.\r\n",
    "        return image\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        \"\"\"\r\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\r\n",
    "        \"\"\"\r\n",
    "        image = self.load_img(self.img[index])\r\n",
    "        label = self.label[index]\r\n",
    "        # label = paddle.to_tensor(label)\r\n",
    "        \r\n",
    "        return data_transforms(image), paddle.nn.functional.label_smooth(label)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \"\"\"\r\n",
    "        步骤四：实现__len__方法，返回数据集总数目\r\n",
    "        \"\"\"\r\n",
    "        return len(self.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_loader\r\n",
    "train_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='train')\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), batch_size=196, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "#val_loader\r\n",
    "val_dataset = MyDataset(train_img_list=train_image_path_list, val_img_list=val_image_path_list, train_label_list=train_label_list, val_label_list=val_label_list, mode='test')\r\n",
    "val_loader = paddle.io.DataLoader(val_dataset, places=paddle.CPUPlace(), batch_size=196, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "print('=============train dataset=============')\r\n",
    "for image, label in train_dataset:\r\n",
    "    print('image shape: {}, label: {}'.format(image.shape, label))\r\n",
    "    break\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # 调用飞桨框架的VisualDL模块，保存信息到目录中。\r\n",
    "# callback = paddle.callbacks.VisualDL(log_dir='visualdl_log_dir')\r\n",
    "\r\n",
    "# from visualdl import LogReader, LogWriter\r\n",
    "\r\n",
    "# args={\r\n",
    "#     'logdir':'./vdl',\r\n",
    "#     'file_name':'vdlrecords.model.log',\r\n",
    "#     'iters':0,\r\n",
    "# }\r\n",
    "\r\n",
    "# # 配置visualdl\r\n",
    "# write = LogWriter(logdir=args['logdir'], file_name=args['file_name'])\r\n",
    "# #iters 初始化为0\r\n",
    "# iters = args['iters'] \r\n",
    "\r\n",
    "# #自定义Callback\r\n",
    "# class Callbk(paddle.callbacks.Callback):\r\n",
    "#     def __init__(self, write, iters=0):\r\n",
    "#         self.write = write\r\n",
    "#         self.iters = iters\r\n",
    "\r\n",
    "#     def on_train_batch_end(self, step, logs):\r\n",
    "\r\n",
    "#         self.iters += 1\r\n",
    "\r\n",
    "#         #记录loss\r\n",
    "#         self.write.add_scalar(tag=\"loss\",step=self.iters,value=logs['loss'][0])\r\n",
    "#         #记录 accuracy\r\n",
    "#         self.write.add_scalar(tag=\"acc\",step=self.iters,value=logs['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/10\n",
      "step 3/3 [==============================] - loss: 0.5436 - acc: 0.6837 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.5187 - acc: 0.9167 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 2/10\n",
      "step 3/3 [==============================] - loss: 0.4956 - acc: 0.9754 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.4394 - acc: 0.9762 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 3/10\n",
      "step 3/3 [==============================] - loss: 0.4067 - acc: 0.9962 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.4317 - acc: 1.0000 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 4/10\n",
      "step 3/3 [==============================] - loss: 0.3779 - acc: 0.9886 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.4196 - acc: 0.9881 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 5/10\n",
      "step 3/3 [==============================] - loss: 0.3649 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3777 - acc: 1.0000 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 6/10\n",
      "step 3/3 [==============================] - loss: 0.3625 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3764 - acc: 1.0000 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 7/10\n",
      "step 3/3 [==============================] - loss: 0.3608 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3718 - acc: 0.9881 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 8/10\n",
      "step 3/3 [==============================] - loss: 0.3547 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3604 - acc: 1.0000 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 9/10\n",
      "step 3/3 [==============================] - loss: 0.3530 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3573 - acc: 1.0000 - 1s/step\n",
      "Eval samples: 84\n",
      "Epoch 10/10\n",
      "step 3/3 [==============================] - loss: 0.3538 - acc: 1.0000 - 3s/step        \n",
      "Eval begin...\n",
      "step 1/1 [==============================] - loss: 0.3672 - acc: 0.9881 - 1s/step\n",
      "Eval samples: 84\n"
     ]
    }
   ],
   "source": [
    "from work.res2net import Res2Net50_vd_26w_4s\r\n",
    "\r\n",
    "# 模型封装\r\n",
    "model_res = Res2Net50_vd_26w_4s(class_dim=4)\r\n",
    "model = paddle.Model(model_res)\r\n",
    "\r\n",
    "# 定义优化器\r\n",
    "\r\n",
    "# scheduler = paddle.optimizer.lr.LinearWarmup(\r\n",
    "#         learning_rate=0.5, warmup_steps=20, start_lr=0, end_lr=0.5, verbose=True)\r\n",
    "# optim = paddle.optimizer.SGD(learning_rate=scheduler, parameters=model.parameters())\r\n",
    "optim = paddle.optimizer.Adam(learning_rate=3e-4, parameters=model.parameters())\r\n",
    "\r\n",
    "# 配置模型\r\n",
    "model.prepare(\r\n",
    "    optim,\r\n",
    "    paddle.nn.CrossEntropyLoss(soft_label=True),\r\n",
    "    Accuracy()\r\n",
    "    )\r\n",
    "\r\n",
    "model.load('work/Res2Net50_vd_26w_4s_pretrained.pdparams',skip_mismatch=True)\r\n",
    "\r\n",
    "# 模型训练与评估\r\n",
    "model.fit(train_loader,\r\n",
    "        val_loader,\r\n",
    "        log_freq=1,\r\n",
    "        epochs=10,\r\n",
    "        # callbacks=Callbk(write=write, iters=iters),\r\n",
    "        verbose=1,\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 保存模型参数\r\n",
    "# model.save('Hapi_MyCNN')  # save for training\r\n",
    "model.save('Hapi_MyCNN2', False)  # save for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, time\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import paddle\r\n",
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "import patta as tta\r\n",
    "\r\n",
    "\r\n",
    "use_gpu = True\r\n",
    "model_file_path=\"Hapi_MyCNN2\"\r\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\r\n",
    "model = paddle.jit.load(model_file_path)\r\n",
    "\r\n",
    "model = tta.ClassificationTTAWrapper(model, tta.aliases.ten_crop_transform(224,224))\r\n",
    "model.eval() #训练模式\r\n",
    "\r\n",
    "def load_image(img_path):\r\n",
    "    '''\r\n",
    "    预测图片预处理\r\n",
    "    '''\r\n",
    "    img = Image.open(img_path).convert('RGB')\r\n",
    "    \r\n",
    "    #resize\r\n",
    "    img = img.resize((224, 224), Image.BILINEAR) #Image.BILINEAR双线性插值\r\n",
    "    img = np.array(img).astype('float32')\r\n",
    "\r\n",
    "    # HWC to CHW \r\n",
    "    img = img.transpose((2, 0, 1))\r\n",
    "    \r\n",
    "    #Normalize\r\n",
    "    img = img / 255         #像素值归一化\r\n",
    "    # print(img)\r\n",
    "    mean = [0.4494708, 0.45852903, 0.29217097]   \r\n",
    "    std = [0.2794664, 0.2744329, 0.2591805]\r\n",
    "    img[0] = (img[0] - mean[0]) / std[0]\r\n",
    "    img[1] = (img[1] - mean[1]) / std[1]\r\n",
    "    img[2] = (img[2] - mean[2]) / std[2]\r\n",
    "    \r\n",
    "    return img\r\n",
    "\r\n",
    "def infer_img(path, model):\r\n",
    "    '''\r\n",
    "    模型预测\r\n",
    "    '''\r\n",
    "    \r\n",
    "\r\n",
    "    #对预测图片进行预处理\r\n",
    "    infer_imgs = []\r\n",
    "    infer_imgs.append(load_image(path))\r\n",
    "    infer_imgs = np.array(infer_imgs)\r\n",
    "    label_list = ['0:優良', '1:良', '2:加工品', '3:規格外']\r\n",
    "    label_pre = []\r\n",
    "    for i in range(len(infer_imgs)):\r\n",
    "        data = infer_imgs[i]\r\n",
    "        dy_x_data = np.array(data).astype('float32')\r\n",
    "        dy_x_data = dy_x_data[np.newaxis,:, : ,:]\r\n",
    "        img = paddle.to_tensor(dy_x_data)\r\n",
    "        out = model(img)\r\n",
    "        # print(out[0])\r\n",
    "        # print(paddle.nn.functional.softmax(out)[0]) # 若模型中已经包含softmax则不用此行代码。\r\n",
    "\r\n",
    "        lab = np.argmax(out.numpy())  #argmax():返回最大数的索引\r\n",
    "        label_pre.append(lab)\r\n",
    "       \r\n",
    "    return label_pre\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_list = os.listdir('data/data97595/nongzuowu/test/testA/')\r\n",
    "img_list.sort(key=lambda x: int(x[5:-4]))\r\n",
    "# img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_list = []\r\n",
    "\r\n",
    "for i in range(len(img_list)):\r\n",
    "    pre_list.append(infer_img(path='data/data97595/nongzuowu/test/testA/' + img_list[i], model=model)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "img = pd.DataFrame(img_list)\r\n",
    "img = img.rename(columns = {0:\"image_id\"})\r\n",
    "img['category_id'] = pre_list\r\n",
    "\r\n",
    "img.to_csv('submit123456.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 提交结果\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3a888bfd6634402e9ae1f42e8b3a9bdf1d3104c9346a4fba9ea51ca8146185ed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
